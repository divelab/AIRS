# Use open-source version of Mamba
_name_: mamba_lm
config:
  _target_: mamba_ssm.models.config_mamba.MambaConfig
  d_model: 128  # Will be overwritten by CL in the scaling exps
  n_layer: 2  # Will be overwritten by CL in the scaling exps
  vocab_size: 12
  pad_vocab_size_multiple: 8
  rms_norm: true
  fused_add_norm: true
  residual_in_fp32: false
  ssm_cfg:
    d_state: 16
    d_conv: 4
    expand: 2
    dt_rank: "auto"
    dt_min: 0.001
    dt_max: 0.1
    dt_init: "random"
    dt_scale: 1.0
    dt_init_floor: 1e-4
    conv_bias: true
    bias: false
    use_fast_path: true
initializer_cfg:
  initializer_range: 0.02
  rescale_prenorm_residual: true
  n_residuals_per_layer: 1
#norm_epsilon: 1e-5  # Default arg in mamba create_block
